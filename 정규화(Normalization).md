## 정규화

http://hleecaster.com/ml-normalization-concept/

#### 데이터의 정규화 이유

> 머신러닝 알고리즘은 데이터가 가진 feature(특성)들을 비교하여 데이터의 패턴을 찾는다.
>
> 여기서 주의할 점은 **데이터가 가진 feature의 스케일이 심하게 차이가 나는 경우 문제**가 된다는 것이다. 특정 feature가 다른 feature들을 완전히 지배하게 되고 다른 feature 정보들을 버리는 꼴이 되버린다. (feature간 스케일 차이가 심하지 않다면 정규화 필요성 감소) 
>
> 모든 데이터 포인트가 동일한 정도의 스케일(중요도)로 반영되도록 해주는 것이 정규화의 목표.

<br/>

#### 정규화 방법

- **Min-Max Normalization(최소-최대 정규화)**

- **Z-Score Normalization(Z-점수 정규화)**

<br/>

1. **Min-Max Normalization(최소-최대 정규화)**

   > 최소-최대 정규화 방법은 데이터를 정규화하는 가장 일반적인 방법이다. 모든 feature에 대해 각각 0~1 범위 안의 값으로 변환하는 것이다.
   >
   > 하지만, 최소-최대 정규화에는 치명적인 단점이 있다. **이상치(Outlier)에 너무 많은 영향을 받는다**는 것이다. 이러한 단점을 보완하기 위해 Z-Score 정규화 방법을 이용한다.

<br/>

2. **Z-Score Normalization(Z-점수 정규화)**

   > **(X - 평균) / 표준편차**
   >
   > 만약 feature의 값이 평균과 일치하면 0으로 정규화되겠지만, 평균보다 작으면 음수, 평균보다 크면 양수로 나타난다. 이 때 계산괴는 음수와 양수의 크기는 그 feature의 표준편차에 의해 결정된다. 그래서 만약 데이터의 표준편차가 크면(값이 넓게 퍼져있으면) 정규화되는 값이 0에 가까워진다.
   >
   > 이상치(Outlier)를 잘 처리하지만, 정확히 동일한 척도로 정규화된 데이터를 생성하지는 않는다.

<br/>

## Decision Tree(결정트리)

https://kolikim.tistory.com/22

#### 장점

- 만들어진 모델을 쉽게 시각화할 수 있어서 비전문가도 이해하기 쉽다.
- 데이터의 스케일에 구애받지 않는다는 것. 각 특성이 개별적으로 처리되어 데이터를 분할하기 때문에, 데이터 스케일의 영향을 받지 않으므로 결정 트리 기반 알고리즘(XGBoost, LightGBM 등)에서는  정규화나 표준화같은 데이터 전처리 과정이 필요 없다.
- 특히, 특성의 스케일이 서로 다르거나 이진 특성과 연속적인 특성이 혼합되어 있을 때에도 잘 작동한다.
- 학습이 끝난 결정 트리의 작업속도가 매우 빠르다.
- 결정 트리 모델이 어떻게 훈련되었는지 경로의 해석이 가능하다.

<br/>

#### 단점

- 사전 가지치기를 사용함에도 과대적합(Overfitting)되는 경향이 있어 모델의 일반화 성능이 좋지 않다는 것이다. 이러한 단일 결정 트리의 단점을 보완한 것이 결정 트리의 앙상블 방법이다.